{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'InverseGamma', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'inverse_gamma', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n",
      "1.张量的创建\n",
      "t: tensor([1., 1., 1., 1.])\n",
      "x: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "x shape: torch.Size([12])\n",
      "y: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "y.numel(): 12\n",
      "z: tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "w: tensor([[[-0.3058, -0.2120,  0.7920,  2.3707],\n",
      "         [-0.0291, -1.7237, -0.8009, -0.0668],\n",
      "         [-1.5987, -1.3963, -0.9308,  0.0695]],\n",
      "\n",
      "        [[ 1.4039, -0.0222, -0.9219,  0.7136],\n",
      "         [-0.1749, -1.5156, -0.0675, -1.3041],\n",
      "         [ 0.8357,  0.6023,  0.8466, -0.2258]]])\n",
      "q: tensor([[1, 2, 3],\n",
      "        [4, 3, 2],\n",
      "        [7, 4, 3]])\n",
      "2.张量的运算\n",
      "tensor([ 3.,  4.,  6., 10.])\n",
      "tensor([-1.,  0.,  2.,  6.])\n",
      "tensor([ 2.,  4.,  8., 16.])\n",
      "tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "tensor([ 1.,  4., 16., 64.])\n",
      "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n",
      "cat操作 dim=0 tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "cat操作 dim=1 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "X == Y tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "X < Y tensor([[ True, False,  True, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "张量所有元素的和: tensor(66.)\n",
      "3.广播机制\n",
      "a: tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "b: tensor([[0, 1]])\n",
      "a + b: tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n",
      "4.索引和切片\n",
      "X: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "X[-1]: tensor([ 8.,  9., 10., 11.])\n",
      "X[1:3]: tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "X: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  9.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "X: tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "5.节约内存\n",
      "False\n",
      "True\n",
      "True\n",
      "6.转换为其他 Python对象\n",
      "<class 'numpy.ndarray'>\n",
      "[[2. 1. 4. 3.]\n",
      " [1. 2. 3. 4.]\n",
      " [4. 3. 2. 1.]]\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n",
      "tensor([3.5000]) 3.5 3.5 3\n"
     ]
    }
   ],
   "source": [
    "# 查看pytorch中的所有函数名或属性名\n",
    "import torch\n",
    "\n",
    "print(dir(torch.distributions))\n",
    "\n",
    "print('1.张量的创建')\n",
    "# ones 函数创建一个具有指定形状的新张量，并将所有元素值设置为 1\n",
    "t = torch.ones(4)\n",
    "print('t:', t)\n",
    "\n",
    "x = torch.arange(12)\n",
    "print('x:', x)\n",
    "print('x shape:', x.shape)  # 访问向量的形状\n",
    "\n",
    "y = x.reshape(3, 4)  # 改变一个张量的形状而不改变元素数量和元素值\n",
    "print('y:', y)\n",
    "print('y.numel():', y.numel())  # 返回张量中元素的总个数\n",
    "\n",
    "z = torch.zeros(2, 3, 4)  # 创建一个张量，其中所有元素都设置为0\n",
    "print('z:', z)\n",
    "\n",
    "w = torch.randn(2, 3, 4)  # 每个元素都从均值为0、标准差为1的标准高斯（正态）分布中随机采样。\n",
    "print('w:', w)\n",
    "\n",
    "q = torch.tensor([[1, 2, 3], [4, 3, 2], [7, 4, 3]])  # 通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值\n",
    "print('q:', q)\n",
    "\n",
    "print('2.张量的运算')\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x ** y)  # **运算符是求幂运算\n",
    "print(torch.exp(x))\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print('cat操作 dim=0', torch.cat((X, Y), dim=0))\n",
    "print('cat操作 dim=1', torch.cat((X, Y), dim=1))  # 连结（concatenate） ,将它们端到端堆叠以形成更大的张量。\n",
    "\n",
    "print('X == Y', X == Y)  # 通过 逻辑运算符 构建二元张量\n",
    "print('X < Y', X < Y)\n",
    "print('张量所有元素的和:', X.sum())  # 张量所有元素的和\n",
    "\n",
    "print('3.广播机制')\n",
    "a = torch.arange(3).reshape(3, 1)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('a + b:', a + b)  # 神奇的广播运算\n",
    "\n",
    "print('4.索引和切片')\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "print('X:', X)\n",
    "print('X[-1]:', X[-1])  # 用 [-1] 选择最后一个元素\n",
    "print('X[1:3]:', X[1:3])  # 用 [1:3] 选择第二个和第三个元素]\n",
    "\n",
    "X[1, 2] = 9  # 写入元素。\n",
    "print('X:', X)\n",
    "\n",
    "X[0:2, :] = 12  # 写入元素。\n",
    "print('X:', X)\n",
    "\n",
    "print('5.节约内存')\n",
    "before = id(Y)  # id()函数提供了内存中引用对象的确切地址\n",
    "Y = Y + X\n",
    "print(id(Y) == before)\n",
    "\n",
    "before = id(X)\n",
    "X += Y\n",
    "print(id(X) == before)  # 使用 X[:] = X + Y 或 X += Y 来减少操作的内存开销。\n",
    "\n",
    "before = id(X)\n",
    "X[:] = X + Y\n",
    "print(id(X) == before)  # 使用 X[:] = X + Y 或 X += Y 来减少操作的内存开销。\n",
    "\n",
    "print('6.转换为其他 Python对象')\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "A = Y.numpy()\n",
    "print(type(A))  # 打印A的类型\n",
    "print(A)\n",
    "B = torch.tensor(A)\n",
    "print(type(B))  # 打印B的类型\n",
    "print(B)\n",
    "\n",
    "a = torch.tensor([3.5])\n",
    "print(a, a.item(), float(a), int(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
