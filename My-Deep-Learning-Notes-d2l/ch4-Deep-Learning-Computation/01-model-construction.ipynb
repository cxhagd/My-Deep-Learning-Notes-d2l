{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3179,  0.0007, -0.0335,  0.0871,  0.1131, -0.0753, -0.0247, -0.1467,\n",
       "         -0.2604, -0.0639],\n",
       "        [-0.2979, -0.1670,  0.0241,  0.0413,  0.0822, -0.1665, -0.0021, -0.0516,\n",
       "         -0.1753, -0.0635]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "\n",
    "X=torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):# 任何一个层都是模组的子类\n",
    "    \"\"\"多层感知机，256个隐藏单元和一个10维输出层\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)# 隐藏层\n",
    "        self.out=nn.Linear(256,10)# 输出层\n",
    "# 定义模型的前向传播，根据输入x返回所需的模型输出\n",
    "    def forward(self, X):    # 把输入放到hidden层\n",
    "        return self.out(F.relu(self.hidden(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0594,  0.0575, -0.0780, -0.1440, -0.1252,  0.1396,  0.1953,  0.1378,\n",
       "          0.0134,  0.1048],\n",
       "        [ 0.0475,  0.0423, -0.1035, -0.1871, -0.0798,  0.1653,  0.1742, -0.0463,\n",
       "         -0.0665,  0.0264]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential的设计是为了把其他模块串起来\n",
    "# 我们需要定义两个关键函数：\n",
    "# 一个将块逐个追加到列表中的函数\n",
    "# 一种前向传播函数  用于把输入按追加块的顺序传递给块组成的链条\n",
    "\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1304, -0.1117,  0.1869, -0.1224,  0.0128,  0.0457, -0.0824,  0.0260,\n",
       "          0.0114,  0.0506],\n",
       "        [ 0.0249, -0.1754,  0.1741, -0.1035,  0.0630, -0.0044, -0.0850,  0.0670,\n",
       "         -0.0902, -0.0030]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential类可以让我们组合新的架构  但是不是所有架构都是简单的顺序架构  当需要更强的灵活性时，我们需要定义自己的块\n",
    "\n",
    "# 有时我没希望合并既不是上一层的结果也不是可更新参数的项\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数  在训练期间保持不变 不会参与训练\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        self.linear=nn.Linear(20,20)\n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X=F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        # 服用全连接层 相当于两个全连接层共享参数\n",
    "        X=self.linear(X)\n",
    "        while X.abs().sum()>1:\n",
    "            X/=2\n",
    "        return X.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0073, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2354, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):#都是子类\n",
    "    def __init__(self):# 把层定义好\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                               nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimera(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
